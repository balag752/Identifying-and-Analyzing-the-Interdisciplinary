{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "import pandas as pd\n",
    "import spacy\n",
    "import re\n",
    "import string\n",
    "\n",
    "from spacy.tokens import Token\n",
    "from tqdm import tqdm\n",
    "\n",
    "from textblob import TextBlob\n",
    "from textblob import Word\n",
    "\n",
    "import html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Importing the file ####\n",
    "Path=\"src/\"\n",
    "Filename='projects_TranslateV2.0.csv'\n",
    "Cat_File=\"category_hier.csv\"\n",
    "df=pd.read_csv(Path+Filename)\n",
    "Cat_data=pd.read_csv(Path+Cat_File)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filtering the empty abstracts.\n",
    "df=df[df[\"Description\"].str.strip()!=\"No abstract available\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    The work of the research training group is dir...\n",
       "1    The subject of the research training group 'Mi...\n",
       "3    The goal of this program consists in the mathe...\n",
       "4    The aim of the research training group is to p...\n",
       "7    Ocean engineering encompasses a wide variety o...\n",
       "Name: Translates, dtype: object"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Translates'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Translates\"]=df[\"Translates\"].apply(str).apply(html.unescape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "ManualStopWords=['e.g.','i.e.','etc.','et al.','ibid.']\n",
    "\n",
    "def remove_spell_errors(doc):\n",
    "    bdoc = TextBlob(str(doc))\n",
    "    ## Correcting the words\n",
    "    return str(bdoc.correct())\n",
    "\n",
    "def stop(doc):\n",
    "    #return [token for token in doc if not token.text in ManualStopWords and len(token.text)>1 and not token.is_digit and not token.is_stop and ( token.text.isalpha() or not token.text.isalnum())]\n",
    "    return [token for token in doc if not token.text in ManualStopWords and len(token.text)>1 and not token.is_digit and not token.is_stop ]\n",
    "\n",
    "def lemmatize(doc):\n",
    "    return [token.lemma_.lower() if token.lemma_ != \"-PRON-\" else token.text.lower() for token in doc]\n",
    "\n",
    "def remove_line_breaks(doc):\n",
    "    return [token.replace(\"\\n\", \" \").replace(\"\\r\", \" \") for token in doc]\n",
    "\n",
    "nlp.add_pipe(stop)\n",
    "nlp.add_pipe(lemmatize)\n",
    "nlp.add_pipe(remove_line_breaks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/52240 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-11-23 16:02:12.958992 : Started preprocessing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 52240/52240 [31:50<00:00, 27.34it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-11-23 16:34:03.719295 : Preprocessing completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(str(datetime.datetime.now())+\" : Started preprocessing\")\n",
    "\n",
    "docs = df['Translates'].apply(str).to_list()\n",
    "processed_docs = []\n",
    "\n",
    "with tqdm(total=len(docs)) as bar:\n",
    "    for doc in nlp.pipe(docs):\n",
    "            line = \" \".join(doc)\n",
    "            ## Removing the punctuation\n",
    "            line=line.translate(str.maketrans('','',string.punctuation))\n",
    "            ## Removing numbers \n",
    "            line=\" \".join(list(filter(lambda w : not w.isdigit(), line.split())))\n",
    "            processed_docs.append(line)\n",
    "            bar.update(1)\n",
    "\n",
    "df[\"PreProcessedDescription\"] = processed_docs\n",
    "print(str(datetime.datetime.now())+\" : Preprocessing completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(Path+'projects_Preprocessed.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
